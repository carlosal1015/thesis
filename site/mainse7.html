<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Method</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html,index=2,3,next,frames --> 
<meta name="src" content="main.tex"> 
<meta name="date" content="2016-07-29 20:29:00"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
   <!--l. 658--><div class="crosslinks"><p class="noindent">[<a 
href="mainse8.html" >next</a>] [<a 
href="mainse6.html" >prev</a>] [<a 
href="mainse6.html#tailmainse6.html" >prev-tail</a>] [<a 
href="#tailmainse7.html">tail</a>] [<a 
href="mainch2.html#mainse7.html" >up</a>] </p></div>
   <h3 class="sectionHead"><span class="titlemark">2.2   </span> <a 
 id="x18-280002.2"></a>Method</h3>
<!--l. 660--><p class="noindent" >I examined the effects of thresholding on image similarity metrics. Specifically, I test the accuracy of
classifying brain images according to their experimental contrast, using several levels of image thresholds
and strategies to handle values that are rendered empty by thresholding. I approach the problem from a
machine learning framework, assessing the accuracy of classifying image contrasts at the varying levels of
thresholding. The results demonstrate that limited thresholding may in some cases have a beneficial effect
on classification accuracy, and that accurate classification can be retained even under fairly aggressive
thresholding.
<!--l. 671--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2.1   </span> <a 
 id="x18-290002.2.1"></a>Data Source</h4>
<!--l. 673--><p class="noindent" >To generate a large set of group maps across many behavioral contrasts, the Human Connectome
Project [<span 
class="ec-lmbx-10">? ? </span>] provides access to large datasets of brain images, including a data release of 501
subjects (including relatives) with the majority having completed seven functional tasks [<span 
class="ec-lmbx-10">? </span>]. The
large number of subjects and assortment of functional paradigms allows for the generation of
samples of unrelated individuals for a wide range of contrasts. From this can be derived an
assessment of the influence of levels of thresholding (Section 2.2.4 Empty voxels in brain statistical
maps) and different strategies for handling empty voxels (Section 2.2.5 Strategies to handle
empty voxels) in a classification framework. Specifically, a study can be done to assess the
influence of image thresholding and the choice of how to handle non-overlapping voxels on the
ability to match any given contrast from one group to the equivalent contrast in a second
group.
<!--l. 689--><p class="indent" >   To generate two groups, A and B, to be used in a random subsampling procedure (Section 2.2.6
Assessing influence of thresholding on classification accuracy),&#x00A0;the HCP data is first subset to the 465 out
of 501 subjects that have data for all contrasts across all tasks. For each of 500 iterations, a random
sampling procedure generates two groups (N = 46) for comparison that ensured no related subjects
between groups. To accomplish this, I take a random sample of 46 subjects for group A, and
randomly&#x00A0;sample&#x00A0;from the remaining subjects, adding to group B only in the case that the sample has no
relations to individuals in group A. I repeat this procedure until I have amassed an appropriately sized
sample.
                                                                                   

                                                                                   
<!--l. 701--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2.2   </span> <a 
 id="x18-300002.2.2"></a>Contrast Selection and Statistical Map Generation</h4>
<!--l. 703--><p class="noindent" >The contrasts are&#x00A0;first filtered to a unique subset. Across the seven functional tasks from HCP (emotion,
working memory, relational, gambling, language, social, and motor), there are a total of 86
contrasts (6, 30, 6, 6, 6, 6, and 26 respectively for each task), and this set is filtered down to
47 (3, 19, 3, 3, 3, 3, 13 respectively) to remove redundancy in the maps, including negation
of contrasts and inverse subtractions (e.g., &#8220;faces - shapes&#8221; vs. &#8220;shapes - faces&#8221;). The list of
contrasts is available in <a 
href="https://github.com/vsoch/thesis/blob/master/supplementary/chapter2/supp_data1_hcp_contrasts_id_filter.csv" >Supplementary Data 2.1</a>. Single subject data for these contrasts is
used to derive group maps for comparison; for each group/contrast, a one-sample t-test is
performed using the FSL randomise tool, which returns a whole-brain t-statistic map. This
procedure results in 47 whole-brain, unthresholded t-statistic maps for each of two unrelated
groups, A and B, for each of 500 iterations. These maps are normalized to Z-scores using an
implementation of Hughetts transform (<a 
href="http://dx.doi.org/10.5281/zenodo.32508" >http://dx.doi.org/10.5281/zenodo.32508</a>) that has better
precision than the tools currently employed in standard neuroimaging software packages [<span 
class="ec-lmbx-10">?</span>
].
<!--l. 721--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2.3   </span> <a 
 id="x18-310002.2.3"></a>Similarity Metrics</h4>
<!--l. 723--><p class="noindent" >While choice of a similarity metric is just as important as a strategy for handling empty voxels, for the
purposes of this study two commonly utilized metrics, Pearson&#8217;s R correlation coefficient, and Spearman&#8217;s
Rank correlation coefficient [<span 
class="ec-lmbx-10">? </span>] were reasonable choices, and implemented with &#8220;pearsonr&#8221; and
&#8220;spearmanr&#8221; in the python package scipy [<span 
class="ec-lmbx-10">? </span>]. The choice of Spearman was important to ensure that
correlation was not influenced by potential outliers in the data.
<!--l. 730--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2.4   </span> <a 
 id="x18-320002.2.4"></a>Empty Voxels in Brain Statistical Maps</h4>
<!--l. 732--><p class="noindent" >As discussed previously, image thresholding introduces empty voxels in brain statistical maps. A set of
thresholds is defined, T, ranging from 0.0 (no threshold applied) to+/-13.0 in increments of 1.0 to cover the
entire range of possible Z-Scores defined for the images (minimum = -12.27, maximum = 11.18). I
consider two separate analyses: first to include positive and negative values, and second to include
only positive values, as researchers interested in positive associations alone may completely
eliminate negative values from a map. In the case of including positive and negative values for a
given threshold, T, the images are thresholded to only include values above +T, and below
                                                                                   

                                                                                   
-T.
<!--l. 742--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2.5   </span> <a 
 id="x18-330002.2.5"></a>Strategies to Handle Empty Voxels</h4>
<!--l. 744--><p class="noindent" >The default of most software is to take one of two approaches: replacing empty voxels with 0, or eliminating
them entirely from the comparison set. These two strategies for handling empty voxels are chosen for this
work to mirror this practice. I first consider data that is only complete, &#8220;complete case analysis&#8221; (CCA).
This means an intersection-based strategy that limits the comparison set to the intersection of non-zero,
non-NaN voxels between two images. Second, I consider the case of single-value imputation (SVI), where
empty/NaN values are replaced with zeros. Each of these two strategies was applied to each of two images
for comparison.
<!--l. 755--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2.6   </span> <a 
 id="x18-340002.2.6"></a>Assessing Influence of Thresholding on Classification Accuracy</h4>
<!--l. 757--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x18-350002.2.6"></a>Extraction of Pairwise Scores</h5>
<!--l. 759--><p class="noindent" >Within each iteration, I calculate pairwise Pearson and Spearman scores for each of the 47 contrasts for
group A (unthresholded) against all 47 contrasts for group B with a particular strategy for handling
empty voxels (CCA and SVI) and threshold applied. Note that because CCA excludes any
voxels not present in both images, it is equivalent to thresholding each map using the non-zero
voxels shared between an unthresholded image A and thresholded image B. For each of the 14
thresholds (including a level of 0.0 that is equivalent to no thresholding applied), I test the
comparisons using both positive and negative values, and only positive values. Using the MNI
standard template brain mask (2 mm), a completely unthresholded image would allow for 228, 483
voxels for comparison. In the cases of no non-zero values surviving a level of thresholding, no
overlapping finite values, or having fewer than three voxels from which to compute a score, I
assert that the maps cannot be compared and thus have no similarity, and ascribe a score of
NaN.
<!--l. 776--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2.7   </span> <a 
 id="x18-360002.2.7"></a>Validation</h4>
                                                                                   

                                                                                   
<!--l. 778--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x18-370002.2.7"></a>Assessment of Empty Voxels on Classification Accuracy</h5>
<!--l. 780--><p class="noindent" >Within each of the 500 subsamples, I make the basic assumption that equivalent contrasts
defined between groups A and B should be most similar, meaning that a particular contrast for
group A should have the greatest similarity with the equivalent contrast for group B across
all contrasts. I can make this assessment for each strategy to handle empty voxels, across all
thresholds, and calculate a mean accuracy for each strategy, threshold, and metric. Specifically:
<br 
class="newline" /><br 
class="newline" />
<!--l. 788--><p class="indent" >   <span 
class="ec-lmbx-10">For each of 500 subsamples: </span><br 
class="newline" />      Subset data to unrelated groups A and B <br 
class="newline" />      For each unthresholded map, A<sub><span 
class="lmmi-7">i</span></sub> in A <br 
class="newline" />         Apply each threshold in Z = +/- 0:13, and Z = + 0:13 to all of B <br 
class="newline" />         Calculate similarity for each of B to A<sub><span 
class="lmmi-7">i</span></sub> <br 
class="newline" />         Assign correct classification if contrast A<sub><span 
class="lmmi-7">i</span></sub> most similar to equivalent contrast in B
<br 
class="newline" /><br 
class="newline" />
<!--l. 796--><p class="indent" >   The &#8220;most similar&#8221; is defined as the highest scoring map from the other group after scores are sorted by
the absolute value in a descending fashion. By comparing the actual vs. the predicted label for each
strategy for handling empty voxels, this evaluation can provide a straightforward assessment of the
influence of empty voxels on image comparison (Figure&#x00A0;<a 
href="#x18-370011">2.1<!--tex4ht:ref: fig:21 --></a>).
<!--l. 803--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                                   

                                                                                   
<a 
 id="x18-370011"></a>
                                                                                   

                                                                                   
<div class="center" 
>
<!--l. 804--><p class="noindent" >

<!--l. 805--><p class="noindent" ><img 
src="images/figure21.png" alt="PIC"  
></div>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;2.1: </span><span  
class="content"> Data generation and analysis process. A subset of 465 datasets from the Human
Connectome Project (subjects) is used to generate 47 contrast maps (group maps) for each of groups
A and B for 500 subsamples. Within each subsample, an unthresholded image from A is compared
with each thresholded image from B with a particular similarity metric and comparison strategy
applied. Each image from A is then assigned the predicted class for the max. arg from the set of B,
and accuracy is calculated for the subsample.</span></div><!--tex4ht:label?: x18-370011 -->
                                                                                   

                                                                                   
<!--l. 808--><p class="indent" >   </div><hr class="endfigure">
                                                                                   

                                                                                   
   <!--l. 810--><div class="crosslinks"><p class="noindent">[<a 
href="mainse8.html" >next</a>] [<a 
href="mainse6.html" >prev</a>] [<a 
href="mainse6.html#tailmainse6.html" >prev-tail</a>] [<a 
href="mainse7.html" >front</a>] [<a 
href="mainch2.html#mainse7.html" >up</a>] </p></div>
<!--l. 810--><p class="indent" >   <a 
 id="tailmainse7.html"></a>    
</body></html> 
