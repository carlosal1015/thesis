<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Experimental definition and capturing of behavior</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html,index=2,3,next,frames --> 
<meta name="src" content="main.tex"> 
<meta name="date" content="2016-07-29 20:29:00"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
   <!--l. 1767--><div class="crosslinks"><p class="noindent">[<a 
href="mainse17.html" >next</a>] [<a 
href="mainap1.html" >prev</a>] [<a 
href="mainap1.html#tailmainap1.html" >prev-tail</a>] [<a 
href="#tailmainse16.html">tail</a>] [<a 
href="mainap1.html#mainse16.html" >up</a>] </p></div>
   <h3 class="sectionHead"><span class="titlemark">A.1   </span> <a 
 id="x30-77000A.1"></a>Experimental definition and capturing of behavior</h3>
<!--l. 1769--><p class="noindent" >Psychological science is concerned with the study of behavior, and logically a behavior cannot be studied
without some kind of measurement. Psychometric theory [<span 
class="ec-lmbx-10">? </span>] provides a conceptual definition for best
practices in measuring observed behavior, and extending to a numerical framework to allow for statistical
analysis of behavior. Typical avenues for collecting behavioral metrics might include asking an individual
about a cognition or experience (self-report), directly observing a phenomenon (observational or field
research), or by direct measurement or control (experimental). While historical assessments of behavior
might involve verbal or written responses (self-report) or manual counting of frequency or
measurement of a desired behavior, modern assessments tend to be digital, typically delivered on the
web or via a digital screen. These experimental paradigms form the basis of modern study of
behavior.
<!--l. 1784--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">A.1.1   </span> <a 
 id="x30-78000A.1.1"></a>Modern behavioral experimentation</h4>
<!--l. 1786--><p class="noindent" >Experimental paradigms are central to psychological science, because they provide the primary means by
which we quantify human behavior. Ideally there would be a coordinated effort to develop and deploy
these paradigms, but unfortunately there is no openly available, extensible coordinated effort,
and thus behavioral datasets tend to be small, and not directly comparable across studies.
The technology available is also limited; while several frameworks have been developed&#x00A0;for
specific steps in the experimentation process (e.g. jsPsych [<span 
class="ec-lmbx-10">? </span>] for experiment creation, Psiturk
[<span 
class="ec-lmbx-10">? </span>] for deployment), these tools require expertise with programming or the command line,
and lack an integrated framework. Additionally, there is currently no large, open repository of
paradigms that can serve as a resource and standard for the field. Without such a resource,
individual labs must either spend unnecessary time time coding tasks, or pay for commercial
products that provide a battery of psychological assessments. The former complicates replication
and&#x00A0;interpretation when reportedly identical paradigms differ in their implementation.The latter
hamstrings&#x00A0;research by providing a particular set of tasks that cannot be extended or modified and that
are limited to laboratories with sufficient resources to pay for them. The success of psychology
requires adoption of modern technology, including instant online access to deploying surveys
                                                                                   

                                                                                   
(e.g., http://www.surveymonkey.com, http://www.qualtrics.com). integration with social media
(e.g., Facebook, Twitter), and a general movement towards a fast, broad collection of data [<span 
class="ec-lmbx-10">?</span>
].
<!--l. 1811--><p class="indent" >   A promising trend in this disorganized landscape is the growth of browser-based experimentation, such
that behavioral paradigms are delivered online or offline through a web browser. &#x00A0;While some
toolboxes are limited to running with specific software [<span 
class="ec-lmbx-10">? ? </span>], there is clear move towards delivery
of experiments over the web using platforms like Amazon Mechanical Turk. The use of the
web necessitates a move to&#x00A0;standard web technologies including JavaScript [<span 
class="ec-lmbx-10">? </span>], HTML, and
CSS [<span 
class="ec-lmbx-10">? ? </span>]. The primary benefit of web-based experimentation is that experiments can be
delivered across platforms (e.g., computers, mobile phones, tablets, fMRI projected screens), and
environments (controlled and uncontrolled settings). Companies have noticed this trend, and there
are a number of pay-for-service products available [<span 
class="ec-lmbx-10">? ? ? </span>]. These products are not ideal for
researchers who generally desire transparency and control over their experiments, and commercial
products are ill-suited for the development of a field-wide repository of shared experimental
implementations. The development of an open-source equivalent, on the other hand, would meet these
requirements.
<!--l. 1830--><p class="indent" >   A wide range of standard infrastructures are in development to help with this task. &#8220;Just Another Tool
for Online Studies&#8221; (JATOS) provides infrastructure to set up a local or server-based set of JavaScript
experiments with a corresponding database, but does not address the issue of standardizing or re-using
paradigms [<span 
class="ec-lmbx-10">? </span>]. The Project Implicit Framework (PIP) [<span 
class="ec-lmbx-10">? </span>] is a modular framework that also deploys
JavaScript experiments, but requires significant expertise to develop and set up components of the
application. Psiturk [<span 
class="ec-lmbx-10">? </span>] is a Python-based web framework (based on the Flask micro-framework) that
researchers can use to develop experiments and deploy on Amazon Mechanical Turk, but is limited to that
implementation, and requires researchers to develop their own paradigms. Finally, Tatool is a web-based
tool for researchers to create and run experiments, offering modern and accessible experiment generation
and analysis. While Tatool is easy to use and a great contribution to open-source experiment
technology, it does not provide standardization to the development of experiments, sharing and
development of a common resource, or integration with existing deployment options (e.g., Psiturk) [<span 
class="ec-lmbx-10">?</span>
].
<!--l. 1850--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">A.1.2   </span> <a 
 id="x30-79000A.1.2"></a>Open source infrastructure to measure behavior</h4>
<!--l. 1852--><p class="noindent" >Open source software for creating experiments has relied upon a small number of libraries and software:
Psych Toolbox [<span 
class="ec-lmbx-10">? </span>] (Matlab), psychopy [<span 
class="ec-lmbx-10">? </span>], jsPsych [<span 
class="ec-lmbx-10">? </span>] (JavaScript). Within this current
system, sharing of code is infrequent, documentation tends to be sparse, formal testing is almost
nonexistent, and paradigms are passed between labs and PIs like protected family heirlooms. The
                                                                                   

                                                                                   
detriments to the research process are extensive. First, independent development of standard
paradigms by multiple individuals leads to redundancy of effort and increases the probability of
coding and conceptual errors in task design. Without open sharing of the experiments, it is not
clear if different results in an analysis are due to a true effect, or an artifact of the different
implementations.&#x00A0;In addition, such practices slow the adoption and vetting of new paradigms once
they are published, since implementation of a new paradigm either requires coding it anew
or acquiring it from the original lab, both of which delay reproduction and extension of the
original work. There is also the potential that untested code can propagate errors across multiple
laboratories. Finally, this potpourri of solutions fails to include any kind of description about how the
data being collected links to cognitive processes of interest, a goal that might be aided by
ontology.
                                                                                   

                                                                                   
   <!--l. 1874--><div class="crosslinks"><p class="noindent">[<a 
href="mainse17.html" >next</a>] [<a 
href="mainap1.html" >prev</a>] [<a 
href="mainap1.html#tailmainap1.html" >prev-tail</a>] [<a 
href="mainse16.html" >front</a>] [<a 
href="mainap1.html#mainse16.html" >up</a>] </p></div>
<!--l. 1874--><p class="indent" >   <a 
 id="tailmainse16.html"></a>   
</body></html> 
