<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>The Academic Software Developer</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html,index=2,3,next,frames --> 
<meta name="src" content="main.tex"> 
<meta name="date" content="2016-07-29 20:29:00"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
   <!--l. 2665--><div class="crosslinks"><p class="noindent">[<a 
href="mainse28.html" >next</a>] [<a 
href="mainse26.html" >prev</a>] [<a 
href="mainse26.html#tailmainse26.html" >prev-tail</a>] [<a 
href="#tailmainse27.html">tail</a>] [<a 
href="mainap3.html#mainse27.html" >up</a>] </p></div>
   <h3 class="sectionHead"><span class="titlemark">C.2   </span> <a 
 id="x43-133000C.2"></a>The Academic Software Developer</h3>
<!--l. 2667--><p class="noindent" >The need for reproducible science has brought with it the emerging niche of the &#8220;academic
software developer,&#8221; an individual that is a combined research scientist and full stack software
developer, and is well suited to develop applications for specific domains of research. In this section,
I will discuss this interesting space that exists between research and software development,
along with the challenges of developing software for reproducible science. As my expertise is in
neuroinformatics, I will provide example by way of tools that I&#8217;ve developed for reproducible neuroimaging
science. In Section 5.2.2, I talk about pyneurovault, cogat-python, and the NIDM-api - all tools
that I have developed during my graduate career to deliver neuroimaging-related databases
directly into applications. I then give a thorough overview of the modern technology required
for developing tools for reproducible research, including data structures, front and back-end
development, and high performance computing. In Section 5.3.5 I focus on virtual machines, and a
particular example of a reproducible workflow, the MyConnectome Project. I then describe a set of
web-based visualization tools that represent the extension of my research into the applied space. In
Section 5.3.6, I discuss pybraincompare, a python module to immediately deploy my semantic
and spatial metrics for image comparison, and visualize comparisons between pairwise maps.
&#x00A0;We start with a review of modern infrastructure and data structures for developing these
applications.
<!--l. 2691--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">C.2.1   </span> <a 
 id="x43-134000C.2.1"></a>Machine Accessibility and Data Structures</h4>
<!--l. 2693--><p class="noindent" >Resources for neuroimaging analysis should&#x00A0;be machine accessible, and publicly available. While many
services for documentation (e.g., Google Docs), file storage (e.g., Amazon S3, Dropbox), and version control
(e.g., Github) offer seamless integration between a local machine and the cloud, this does not guarantee
that these resources are programmatically accessible. While it may be possible to move files and data by
way of a command line, content such as free text in documents and meta-data about files that is not
captured in the researcher&#8217;s workflow is lost forever. &#x00A0;The goal of making resources machine
accessible by way of standard data structures is highly challenging in that there is a tradeoff
between complexity and usability. A data structure to describe a resource that is developed in
a way to capture every detail may be highly challenging to integrate into applications, and
                                                                                   

                                                                                   
turn developers away from using it. On the other hand, too simple a data structure may fail
to capture meaningful meta-data about the resource(s) it intends to describe. Ideally, these
standard data structures should be intuitive, easy to use, and modern. In this section, I will
provide examples of machine accessible resources that vary in the degree to which this goal is
met.
<!--l. 2713--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">C.2.2   </span> <a 
 id="x43-135000C.2.2"></a>Application Programming Interfaces for Neuroimaging Research</h4>
<!--l. 2715--><p class="noindent" >The previously discussed NeuroVault database for obtaining whole-brain statistical maps, or the Cognitive
Atlas for providing a description of cognitive neuroscience, are not useful in applications if they existed
only as websites. Machine accessibility usually comes by way of an API, or an application
programming interface. Further, this API must be developed to provide a data structure that
encourages its use in the development of applications for research because it is intuitive, easy,
and modern. One of my first goals in my early graduate career was to develop these APIs for
the databases and web resources that I knew were important for distribution to the larger
public.
<!--l. 2726--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-136000C.2.2"></a>pyneurovault</h5>
<!--l. 2728--><p class="noindent" >The first step to developing an API is considering the two domains that are being connected. Neuroimaging
science is moving heavily toward the use of Python, or &#8220;pythonic&#8221; tools, and so it is a logical decision
that the NeuroVault database uses the Django (<a 
href="http://www.djangoproject.org" >http://www.djangoproject.org</a>) framework
as a back-end, as it is a python framework and will allow for seamless integration of these
tools into the functions of the database. This initial step can be seen as a strategy to integrate
the tools that neuroimaging scientists are developing into the larger framework of the web.
The next question is how to properly distribute the NeuroVault database as a repository of
brain maps, and for this I have developed a two-pronged strategy. First, NeuroVault must be
able to provide both researcher and other web applications with access to metadata about
images, and file locations to download images. Toward this goal, it was decided to provide
a RESTful (representational state transfer) API [<span 
class="ec-lmbx-10">? </span>], which means using hypertext transfer
protocol (HTTP) or urls to send (POST) and retrieve (GET) data. What this means is that
NeuroVault provides a set of formatted URLs (<a 
href="http://neurovault.org/api-docs" >http://neurovault.org/api-docs</a>) that users can use to
query the database, and retrieve results in the JSON (JavaScript Object Notation) format.
For a researcher who is comfortable with using different APIs, it is common knowledge that
these kind of APIs can integrate seamlessly into analysis pipelines in almost any language
simply by retrieving the web page, and parsing the JSON object into a data structure that is
                                                                                   

                                                                                   
familiar to the language and researcher. Other web applications, by default of the domination
of JavaScript over the internet, would be able to easily integrate this resource a well. As an
additional step, due to the fact that a large portion of neuroimaging researchers are comfortable
working in python, I developed pyneurovault (https://github.com/NeuroVault/pyneurovault), a
python wrapper for this RESTful API so that functions are designed to form the RESTful
queries, and convert filtering and specification of various parameters into easy to use functions in
python.
<!--l. 2762--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-137000C.2.2"></a>cogat-python</h5>
<!--l. 2764--><p class="noindent" >The Cognitive Atlas, by way of being an older database, includes a more standard RESTful
API based on a third-party language converting SQL commands to the relational database
to output the equivalent JSON data structures. I developed an equivalent python wrapped,
cogat-python (cogat-python.readthedocs.org) to immediately empower neuroimaging scientists most
comfortable with python to query the concepts, tasks, and contrasts in the atlas. This API
was released early 2015, and has been the driver behind the body of work presented in this
thesis. Interestingly, before the development of the RESTful API and python wrapper, the
Cognitive Atlas was equipped with a form of semantic technology for users and developers. This
technology, discussed in the next section, provides a clear example for how a web resource must be
matched to its users and easily integratable into analysis frameworks, otherwise it will not be
used.
<!--l. 2779--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-138000C.2.2"></a>expfactory-analysis</h5>
<!--l. 2781--><p class="noindent" >Akin to NeuroVault and the Cognitive Atlas, the Experiment Factory Docker infrastructure is a web
interface that holds behind it a mine of behavioral data. An additional challenge in this case is the fact that
this data might contain identifying information about participants that participated in a battery on
Amazon Mechanical Turk, and should not be publicly available. To address this concern and still provide a
RESTful API to serve results programatically, the application allows users with an account to generate
tokens that can be embedded in the request from any application to use it. To make this process easy, I
developed a Python wrapper to the Experiment Factory RESTful API (expfactory-analysis, available at
<a 
href="https://github.com/expfactory/expfactory-analysis" >https://github.com/expfactory/expfactory-analysis</a>) that, in one line, will retrieve paginated results given
an acceptable token:
<!--l. 2795--><p class="indent" >   from expanalysis.api import get_results
<!--l. 2797--><p class="indent" >   access_token = &#8220;abcdefghijklmnopqrstuvwxyz&#8221;
<!--l. 2799--><p class="indent" >   results = get_results(access_token=access_token)
                                                                                   

                                                                                   
<!--l. 2801--><p class="indent" >   While security and protection of personal health information (PHI) is outside the scope of this review,
it is an essential detail to many sources of data, and care should be taken to ensure protection of private
data when generating data structures and tools to query them.
<!--l. 2806--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">C.2.3   </span> <a 
 id="x43-139000C.2.3"></a>The Vision of the Semantic Web</h4>
<!--l. 2807--><p class="noindent" >The semantic web is a set of standards released by the World Wide Web consortium for data structures
and standards [<span 
class="ec-lmbx-10">? </span>] that can be used to turn the internet into a web of data, instead of a web of documents.
The basic data structure that defines this standard is the Resource Description Framework (RDF), which
can be thought of as a text file that defines resources, and then describes how they relate to one another,
typically with the format of:
<!--l. 2815--><p class="indent" >   resource1 &#8211;&#x003E; relationship &#8211;&#x003E; resource2
<!--l. 2817--><p class="indent" >   These relationships, called &#8220;RDF triples&#8221; form the basis of a graph. The vision of the semantic web is a
beautiful one [<span 
class="ec-lmbx-10">? </span>]:
<!--l. 2825--><p class="indent" >   <span 
class="ec-lmri-10">The overall vision of the Data Activity is that people and organizations should be able to share data as</span>
<span 
class="ec-lmri-10">far as possible using their existing tools and working practices but in a way that enables others to derive and</span>
<span 
class="ec-lmri-10">add value, and to utilize it in ways that suit them. Achieving that requires a focus not just on the</span>
<span 
class="ec-lmri-10">interoperability of data but of communities.</span>
<!--l. 2827--><p class="indent" >   Unfortunately, there is a wide gap between the developers of this standard, and its application. RDF is
an example of a data structure that has a steep learning curve for use, and requires substantial effort by
semantic web folk to make its contents readily accessible to standard web developers. The main language to
parse these documents, &#8220;sparql&#8221; [<span 
class="ec-lmbx-10">? </span>], does not bring with it an intuitive understanding and easy of use that
is prevalent in a language like python. While it could be that this vision is sound and simply more work is
needed to refine the standard, this author believes that the standard is not being more integrated
into web applications because it&#8217;s just too hard. These standards are struggling not due to a
lack of time and care put into developing them, but because they do not easily extend to easy
integration into the workflows and practices that modern developers are accustomed to. &#x00A0;As
a web developer, or a researcher that heavily uses Python, the first thought that comes to
mind when being exposed to a new data structure should not be &#8220;How do I get it out of this
format?&#8221; &#x00A0;This is an unfortunate reality, and a risky pursuit to push such a data structure
that stands in sheer opposition to the way that people naturally think about modeling the
world.
<!--l. 2847--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-140000C.2.3"></a>The Neuroimaging Data Model (NIDM)</h5>
                                                                                   

                                                                                   
<!--l. 2849--><p class="noindent" >An example of a set of models that use RDF as data structures in neuroimaging is the The Neuroimaging
Data Model (NIDM) [<span 
class="ec-lmbx-10">? </span>]. NIDM&#x00A0;is an extension of some of the World Wide Web consortium&#8217;s models to
describe the outputs of neuroimaging analyses, including Results, Experiments, and Workflows. The vision
of the NIDM working group is strong in that the standard aims to&#x00A0;define a standard data
structure to export the output of an analysis, and extend to other applications. Recognizing
this importance and the difficulty of using RDF, I devoted a portion of my graduate career
to the development of web-based tools for neuroimaging scientists that integrates these data
structures.
<!--l. 2860--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-141000C.2.3"></a>The NIDM-Viewer</h5>
<!--l. 2862--><p class="noindent" >The object model called &#8220;NIDM-Result&#8221; is a data structure produced from analysis software that
describes the parameters used in derivation of a statistical brain map result. &#x00A0;For example,
an NIDM data structure that goes along with a statistical brain map would describe peak
coordinates, their values, processing choices to describe the maps, and links to all other related
maps. This information would be useful to integrate into a web application, but tools to parse
the information from RDF itself and deliver the information in a web interface had not been
developed.
<!--l. 2872--><p class="indent" >   Using the NIDM Results data structure, my first effort was to develop an NIDM-Viewer than can
immediately visualize brain maps and associated significant coordinates in a web browser. I
created the NIDM-Viewer (<a 
href="http://vsoch.github.io/nidmviewer" >http://vsoch.github.io/nidmviewer</a>), which can be run from a local
machine or on a server to visualize and browse the result of a neuroimaging analysis. The
back-end technology to produce this output was a Flask (Python) application, meaning that the
Python modules could also be integrated into pythonic tools (e.g., Django). Thus, a logical next
step was integration of this viewer into the NeuroVault database (implemented in Django)
so that researchers sharing NIDM-Results can immediately and interactively browse these
results without needing to know the Sparql query language or interact with the data structure
(Figure&#x00A0;<a 
href="#x43-141000C.2.3">C.2.3<!--tex4ht:ref: fig:51 --></a>).
<!--l. 2886--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                                   

                                                                                   
                                                                                   

                                                                                   
<div class="center" 
>
<!--l. 2887--><p class="noindent" >

<!--l. 2888--><p class="noindent" ><img 
src="images/figure51.png" alt="PIC"  
></div>
<!--l. 2890--><p class="noindent" > <span 
class="ec-lmbx-10">Figure 5.1  </span>The NIDM Viewer used Python to parse three dimensional brain coordinates and statistical
result values from locations in the brain map into an interactive table alongside the brain map
itself.
                                                                                   

                                                                                   
<!--l. 2894--><p class="indent" >   </div><hr class="endfigure">
<!--l. 2896--><p class="indent" >   This is an example of distributing an easy to use tool that immediately empowers a neuroimaging
researcher to compare his or her result to others, as the viewer can render any set of NIDM-Results files
that are available to it.
   <h5 class="subsubsectionHead"><a 
 id="x43-142000C.2.3"></a>The NIDM-api</h5>
<!--l. 2903--><p class="noindent" >While the NIDM-Viewer initially worked by way of embedding sparql queries into the application itself,
this workflow was extremely challenging from a development standpoint. The core issue was that
integration of Sparql queries into a browser, to query RDF and generate a web-friendly data structure
(JSON), was not standardized. The workflow that was required to generate the viewer, specifically having
expertise to write Sparql queries and run them in Python, would never be tolerated or even possible for a
standard web developer. I had the insight that the proper way to address this informatics problem would
be to serve the queries in a format that is tolerable and easy. My solution was to develop an open source
framework that would separate serve the results of Sparql queries by way of a RESTful API
(<a 
href="http://nidm-api.readthedocs.org" >http://nidm-api.readthedocs.org</a>) using a python web framework ([<span 
class="ec-lmbx-10">? </span>]), the NIDM-api. The second insight
was that the queries themselves should be served in an intuitive data structure (JSON), and
maintained separately from the software to serve them to allow for specialization of expertise. The
NIDM-api (manuscript under review) can dually provide a tool to serve Sparql queries against RDF
documents, and interactive web interfaces for generating new query objects and visualizing
results.
<!--l. 2925--><p class="indent" >   The logic behind this application is that the software cleanly separates the the queries themselves from
the application serving them by way of different Github repositories. Upon starting the application, the
most recent set of queries is programatically obtained from Github, and served to the user both in
interactive web interfaces and by way of a RESTful API. The query repository can be maintained by the
experts in the semantic web, and the framework to serve them by developers that may not have expertise
with sparql or RDF. Individuals with expertise in Sparql but possibly not programming skills can generate
and interact with queries via an interactive web interface (Figure&#x00A0;<a 
href="#x43-142000C.2.3">C.2.3<!--tex4ht:ref: fig:52 --></a>) and developers with
no expertise in sparql that want to perform queries don&#8217;t have to see or know that it exists.
&#x00A0;
<!--l. 2938--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                                   

                                                                                   
                                                                                   

                                                                                   
<div class="center" 
>
<!--l. 2939--><p class="noindent" >

<!--l. 2940--><p class="noindent" ><img 
src="images/figure52.png" alt="PIC"  
></div>
<!--l. 2942--><p class="noindent" > <span 
class="ec-lmbx-10">Figure 5.2  </span>The NIDM-api includes an interactive web interface to generate query data structures to
serve in the Github repo for queries, nidm-query.
                                                                                   

                                                                                   
<!--l. 2945--><p class="indent" >   </div><hr class="endfigure">
<!--l. 2947--><p class="indent" >   This ensures that, if a software developer outside of the NIDM group wants to develop an application
that makes inferences over these data structures, he or she will not have to know or understand Sparql or
RDF to do so. For academic software developers, the lesson to be learned is to develop for the now, not for
the idealized future.
   <h4 class="subsectionHead"><span class="titlemark">C.2.4   </span> <a 
 id="x43-143000C.2.4"></a>Frontend and Backend Web Frameworks</h4>
<!--l. 2955--><p class="noindent" >The development of tools for researchers, along with visualization of documentation and results, requires
both expertise for writing code on web or server for analysis (a &#8220;back-end&#8221;) or code that is rendered
directly in a web browser for the user (&#8220;front-end&#8221;). Thus, an academic software developer must have
expertise in both these domains.
<!--l. 2961--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-144000C.2.4"></a>Back End Frameworks</h5>
<!--l. 2963--><p class="noindent" >The back-end framework refers to the organization and choice of databases, machines, and any operation
that must be run on a server to ensure the proper function of an application. These operations are typically
not &#8220;seen&#8221; by the user, and require design choices that take into account the anticipated usage of the
application, security and redundancy, and the applications themselves [<span 
class="ec-lmbx-10">? </span>]. For example, when deploying a
reproducible workflow, the MyConnectome Project (Section 5.2.7), Elastic Load Balancing, a strategy to
deploy an application on several servers and direct incoming traffic to be balanced over the servers,
was needed to ensure consistent application availability. &#x00A0;Servers to host web applications
must also make choices about whether to server secure (SSL) connections. For example, the
Experiment Factory integrates with Amazon Mechanical Turk, and a requirement for such an
application is that it provides a sure (HTTPS) connection. Such an application that will be used
concurrently by many users would also need a database optimized for this concurrency, such
as postgreSQL, ideally with encrypted data transfer between the application and database.
To achieve this, it was necessary to serve the encrypted connection (typically on port 443)
by way of authenticating the server with SSL certificates. This means that any user browser
navigating to the page has assurance that the connection is encrypted, and any data transfer is
protected.
<!--l. 2986--><p class="indent" >   The core of web-based applications includes several back-end components. Servers that
are accessible on the internet have been assigned to it an IP (internet protocol) address that
can be found by other machines, and have a web server open (typically at port 80 or port
443 for secure connections) with permission to allow incoming traffic. A typical application
carries with it data, and so a relational database [<span 
class="ec-lmbx-10">? ? </span>] is the bread and butter of modern
data storage, and in some cases external e services [<span 
class="ec-lmbx-10">? ? </span>] are appropriate for retrieval of static
                                                                                   

                                                                                   
files. There are various choices for web servers [<span 
class="ec-lmbx-10">? </span>], as there are machines and hosts [<span 
class="ec-lmbx-10">? </span>], and a
developer looking to configure a machine typically makes his or her choice based on price,
availability of development tools [<span 
class="ec-lmbx-10">? </span>], and previous experience. The take home point is that
the setup of the back-end technology for any application or service must be done with care
and concern for the data being served, the users, and the longevity and affordability of the
application.
<!--l. 3004--><p class="noindent" ><span class="paragraphHead"><a 
 id="x43-145000C.2.4"></a><span 
class="ec-lmbx-10">Application Frameworks</span></span>
   The back end framework can be seen as a bassinet to give security and stability to that which it is
meant to take care of, the application itself. An application framework refers to the libraries of functions
that perform operations on the data, and generate a result for the frontend application, the part of the
infrastructure that is &#8220;seen&#8221; by the user. Choice of an application framework is incredibly important as it
can streamline development. For early applications, the standard was to write custom php [<span 
class="ec-lmbx-10">? </span>]
functions to work with a relational database and return queries to the front end. These scripts
developed into standardized content management systems (CMS) that cleverly packaged base
infrastructures and functions to work with databases, giving developers the ability to write
custom script &#8220;plugins&#8221; to extend use of the application. My earliest work in graduate school
(<a 
href="http://vbmis.com/bmi/noisecloud" >http://vbmis.com/bmi/noisecloud</a>) provided a database of spatial and temporal brain imaging features
associated with artifact in functional MRI accessible via an API by way of a CMS called Wordpress [<span 
class="ec-lmbx-10">? </span>].
This framework&#x00A0;is estimated to encompass al most 60% of all CMS, and approximately a quarter of all
websites on the internet [<span 
class="ec-lmbx-10">? </span>]. However, the lesson I learned from this early experience is that
Wordpress is a blogging platform, and not well suited for research applications that require more
than sharing of text and image content. I realized quickly that Python-based [<span 
class="ec-lmbx-10">? ? </span>], or server
side JavaScript [<span 
class="ec-lmbx-10">? </span>] were more modern, flexible choices, empowering the user to create more
custom user experiences. Early in my graduate school career, tools for easy deployment of data
analysis content was not properly developed, and during this short time the statistics community
that uses software called R has made substantial progress to deliver these applications on the
web via a shiny server [<span 
class="ec-lmbx-10">? </span>]. Shiny&#x00A0;has become a popular avenue for data scientists to publish
interactive charts, however several issue remains. Hosting of such applications is still limited
[<span 
class="ec-lmbx-10">? </span>] and expensive, and the server itself not easy to set up. It would be highly valued for a
University or academic institution to provide hosting, or minimally an avenue to serve these
research-oriented applications. Shiny is an example of just one framework, and an academic software
developer must be aware of and comfortable working with the most modern frameworks that can
easily transform data and scientific result into an interactive web-based experience. Beyond
expertise in setting up a server and coding an application, this requires front-end expertise as
well.
                                                                                   

                                                                                   
<!--l. 3048--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-146000C.2.4"></a>Front End Resources</h5>
<!--l. 3050--><p class="noindent" >The front-end framework refers to the library of functions and scripts that define the user experience, which
is typically text, images, and interactive content rendered on a web page. The base of this
content is HTML5 [<span 
class="ec-lmbx-10">? </span>], which is not a language but a syntax that a browser understands how to
render to properly display content. The style of a page comes by way of cascading style sheets
(CSS3) [<span 
class="ec-lmbx-10">? </span>], and dynamic content most typically comes by way of JQuery [<span 
class="ec-lmbx-10">? </span>], a javascript
library that is used by over 60% of the top 100,000 websites [<span 
class="ec-lmbx-10">? </span>]. While the front-end design
may seem less important than the application itself, the user experience is an often overlooked
essential component in software design that can make or break the success of an application.
There are several resources for front-end developers that combine components of front-end
design [<span 
class="ec-lmbx-10">? ? ? </span>], and during my time in graduate school I found myself devoting much time to
learning these frameworks to provide modern visualizations to accompany my research.&#x00A0;I quickly
found it obvious that these resources were not developed specifically for research scientists, and
attributed the lack of equivalent resources for this purpose due to the inability of the academic
landscape to attract and keep experienced developers. For example, there is a web font,&#x00A0;&#8220;font
awesome,&#8221; that provides standard web icons that are hidden among most websites. There are no
statistical symbols or icons related to specific scientific domains provided in this library, and as
an early graduate student I saw no reason that resources of this caliber could not exist for
researchers. During my graduate career I developed a web font, &#8220;font-brain&#8221; that would provide
meaningful symbols for neuroimaging scientists to embed into web applications, manuscripts, or code
repositories (<a 
href="https://vsoch.github.io/font-brain" >https://vsoch.github.io/font-brain</a>). While Font-brain&#8217;s usage is limited to a set
of my applications, it is an example of how modern resources are needed for brain imaging
scientists.
<!--l. 3083--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">C.2.5   </span> <a 
 id="x43-147000C.2.5"></a>Development Architecture</h4>
<!--l. 3085--><p class="noindent" >Development of applications requires many individuals working collaboratively on a common code base. it
requires careful, regular testing of code (continuous integration), along with generation of a consistent
development environment for developers to ensure that server architecture matches development
architecture. In this section I will cover standard routine for collaborating on a code base (version control),
along with development workflows and container-based architectures.
<!--l. 3094--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-148000C.2.5"></a>Continuous Integration</h5>
                                                                                   

                                                                                   
<!--l. 3095--><p class="noindent" >As previously discussed, Github is an essential choice for software developers to collaborate on
coding projects in that it provides version control. &#x00A0;An application&#8217;s code can live in a central
repository (repo), and the repository has different branches, or versions of the application that
are being actively developed. The repo typically has a &#8220;master&#8221; branch where the production
quality application is stored, and making changes to this branch is done by way of a &#8220;pull
request,&#8221; (PR) which is a process of comparing a modified branch&#8217;s code to the master, and
having a discussion about the changes. The PR is typically set up to automatically perform
&#8220;continuous integration,&#8221; or running a series of tests over all of the proposed changes to make
sure that no functionality has been broken by the changes. Continuous integration (CI) is
provided to developers as a service [<span 
class="ec-lmbx-10">? ? </span>], and can be used to test the application on different
system setups, as well as render application outputs that are static files. Writing good tests
is an art in itself, and another important step in developing applications that aim to have
longevity.
<!--l. 3113--><p class="indent" >   While the base case of CI is to test a code base, it should not go unnoticed that such a service will allow
for running a snippet of code over any function of interest in a github repository. While not the original
intention, a service like continuous integration would allow for a researcher to share code, and if another
researcher copies (forks) the repo, the code could be run immediately to produce an output.
&#x00A0;
<!--l. 3120--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x43-149000C.2.5"></a>Container-based Architectures</h5>
<!--l. 3122--><p class="noindent" >A modern description of web technology would not be complete without a discussion of container-based
architecture. Docker [<span 
class="ec-lmbx-10">? </span>] has emerged as a promising framework to streamline development for reproducible
research [<span 
class="ec-lmbx-10">? </span>]. and development workflows. Docker is based on the idea of creating different &#8220;containers&#8221; for
each component of an application, and a container can encompass any single entity that can be
installed on a server. Containers can be shared [<span 
class="ec-lmbx-10">? </span>] to allow for reproducible workflows, and
utilize a system&#8217;s resources in a more resourceful way than installation of the equivalent number
of virtual machines [<span 
class="ec-lmbx-10">? </span>]. Developing an application with Docker means that it can easily be
deployed anywhere with root permissions [<span 
class="ec-lmbx-10">? </span>], and while this does not extend well to servers or
high performance computing (HPC) environments where this is not possible, it is a step in
the right direction to reproducible systems, and an improvement over previous methods [<span 
class="ec-lmbx-10">?</span>
].
<!--l. 3140--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">C.2.6   </span> <a 
 id="x43-150000C.2.6"></a>High Performance Computing</h4>
                                                                                   

                                                                                   
<!--l. 3142--><p class="noindent" >A major challenge to developing applications for science is the need for high performance computing (HPC)
[<span 
class="ec-lmbx-10">? </span>]. HPC is the use of massive grids of machines, connected together with a job management system [<span 
class="ec-lmbx-10">? ? </span>]
to run jobs that require high computational resource such a memory or storage in parallel. Most major
universities give researchers access to a grid of these machines, called a &#8220;cluster,&#8221; whether for free or for a
price. HPC can render an analysis that would take 8 months to run in serial done within an hour in
a parallel environment. The biggest challenge for researchers is not using HPC, but rather,
extending HPC to work in a cloud environment [<span 
class="ec-lmbx-10">? </span>], integrated into a web application. This is a
burgeoning area of development with much promise [<span 
class="ec-lmbx-10">? </span>]. Further, the HPC environment is not yet
friendly to container-based architectures that can extremely improve reproducibility, discussed
next.
<!--l. 3159--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">C.2.7   </span> <a 
 id="x43-151000C.2.7"></a>Virtual Machines</h4>
<!--l. 3161--><p class="noindent" >The choice of a back and front-end web framework, as well as a strategy for collaborative development and
continuous integration says nothing about application deployment. While use of a hosted web server seems
like the ideal solution to share an application, it may not be appropriate for a research product such as an
analysis pipeline. Further, analysis pipelines are complex entities, often requiring the use of
multiple programming languages with different libraries, access to large data, and memory. In this
context, a reasonable solution, and on par with a container architecture, would be to package a
configuration into a virtual machine (VM). Virtualbox [<span 
class="ec-lmbx-10">? </span>] is a leader in virtual machine deployment
software, with wrappers such as Vagrant [<span 
class="ec-lmbx-10">? </span>] allowing for command line control of machines.
These virtual machines can not only deploy applications, they can also be used for development
environments. With Vagrant, a single file called a Vagrantfile is run to bring up an entire virtual
machine, and such a file can be shared easily on a service like Github. A virtual machine can be
generated and deployed on a user&#8217;s local machine, or straight to a cloud service like Amazon
Web Services (AWS). This kind of immediate packaging of an analysis holds great promise
for distributing reproducible workflows, and during my time at Stanford I have worked on
deploying several reproducible workflows in this fashion. A noteable example is the MyConnectome
Virtual Machine (<a 
href="https://github.com/poldrack/myconnectome-vm" >https://github.com/poldrack/myconnectome-vm</a>), which integrates genomic,
metabolomic, behavioral, and brain imaging data from multiple sources into an analysis that can be
immediately deployed to the cloud (<a 
href="http://results.myconnectome.org" >http://results.myconnectome.org</a>) or on a user&#8217;s local machine
(Figure&#x00A0;<a 
href="#x43-1510011">C.1<!--tex4ht:ref: fig:53 --></a>).
<!--l. 3191--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                                   

                                                                                   
<a 
 id="x43-1510011"></a>
                                                                                   

                                                                                   
<div class="center" 
>
<!--l. 3192--><p class="noindent" >

<!--l. 3193--><p class="noindent" ><img 
src="images/figure53.png" alt="PIC"  
></div>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;C.1: </span><span  
class="content"> The MyConnectome virtual machine was a successful strategy to completely reproduce
a complicated analysis involving brain imaging, genomics, behavior, and metabolomic data.</span></div><!--tex4ht:label?: x43-1510011 -->
                                                                                   

                                                                                   
<!--l. 3198--><p class="indent" >   </div><hr class="endfigure">
<!--l. 3200--><p class="indent" >   The core challenge was bringing together disparate data sources into one computational environment,
running complicated, time-intensive analysis, and keeping the user updated about progress and errors. A
solution was needed to obtain data, process it, and render a result for the user. My advisor Russell
Poldrack approached me when he started developing this pipeline, and challenged me to create an interface
to integrate into a virtual machine to interact with it. &#x00A0;This task proved to be much more challenging than
anticipated. We quickly learned that the failure of a single download would put a stop in the entire
pipeline, which could take upwards of 12 hours depending on internet connectivity, and it would be
required for the user to have command line expertise to ssh into the virtual machine to debug or look for
logs that might hint about what went wrong. It was immediately clear that we needed some form of
communication of error and console output, and that the front-end user interface must dynamically
update as the back-end analyses are completed. First, to make the generation of results on the
virtual machine transparent to the user, I linked all outputs to links on the main interface,
and these links would activate and change from gray to green when the result was produced.
To keep the user alert to the timing of analyses, I generated logs of expected time based on
results that were generated, and wrote a function to reflect this timing in a progress bar on the
main interface. The lesson here is that there must be accurate communication between a user
and a tool, otherwise the tool is not serving its purpose to reflect what is happening server
side.
<!--l. 3225--><p class="indent" >   Finally, while many results were already web-friendly (e.g., ipython notebook or R-markdown rendered
into HTML), many results were tab delimited files that were not. My goal was to use modern web
technology to render both results and output and error logs in a meaningful fashion. Toward this goal, I
created an interactive interface to render error and output logs, and select result files from intuitively
organized drop down menus, and render in a sortable, clean table. The final addition of a single download
for all results completed the virtual machine. The complexity of this project brought to light that deploying
a reproducible workflow, as a custom application, is incredibly challenging, and must more care and
thought must be put into designing tools that make this process no more than the click of a
button.
                                                                                   

                                                                                   
   <!--l. 3239--><div class="crosslinks"><p class="noindent">[<a 
href="mainse28.html" >next</a>] [<a 
href="mainse26.html" >prev</a>] [<a 
href="mainse26.html#tailmainse26.html" >prev-tail</a>] [<a 
href="mainse27.html" >front</a>] [<a 
href="mainap3.html#mainse27.html" >up</a>] </p></div>
<!--l. 3239--><p class="indent" >   <a 
 id="tailmainse27.html"></a>   
</body></html> 
