<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>The Reproducibility Crisis</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html,index=2,3,next,frames --> 
<meta name="src" content="main.tex"> 
<meta name="date" content="2016-07-29 20:29:00"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
   <!--l. 2456--><div class="crosslinks"><p class="noindent">[<a 
href="mainse27.html" >next</a>] [<a 
href="mainap3.html" >prev</a>] [<a 
href="mainap3.html#tailmainap3.html" >prev-tail</a>] [<a 
href="#tailmainse26.html">tail</a>] [<a 
href="mainap3.html#mainse26.html" >up</a>] </p></div>
   <h3 class="sectionHead"><span class="titlemark">C.1   </span> <a 
 id="x42-128000C.1"></a>The Reproducibility Crisis</h3>
<!--l. 2458--><p class="noindent" >Making inferences from image comparisons requires much more than the existence of ontologies and
methods, it calls for the consistent practice of reproducible research. As current media has suggested [<span 
class="ec-lmbx-10">? ? ?</span>
                                                                                   

                                                                                   
], the reproducibility of neuroimaging and cognitive science is messy at best. Reproducibility goes far
beyond the creation of a single database to deposit results, and factors such as careful documentation of
variables and methods [<span 
class="ec-lmbx-10">? ? </span>], how the data were derived [<span 
class="ec-lmbx-10">? ? ? </span>], and dissemination of results [<span 
class="ec-lmbx-10">? ? </span>] unify to
embody a pattern of sound research practices that have previously not been emphasized. Any single step in
an analysis pipeline that is not properly documented, or does not allow for a continued life cycle of a
method or data, breaks reproducibility. In this reality, it is clear that long term success in practicing
reproducible science is asking much more than domain expertise and publication from our
researchers.
<!--l. 2474--><p class="indent" >   The neuroscience community was shamed in 2013 when it was suggested [<span 
class="ec-lmbx-10">? </span>] that studies in
neuroscience were lacking statistical power, and failing the most fundamental goal of science: to contribute
evidence for a discovery and replicate the result. While large-scale efforts are contributing datasets of
substantial size ([<span 
class="ec-lmbx-10">? ? </span>]) to allow for the critical assessment of findings in neuroscience, a much more
substantial problem is the habits and standards that are acceptable for researchers to partake in from the
initial generation of an idea through the publishing of a completed manuscript. While a great burden
must be placed on the researchers to design sound experiments, conduct proper statistical
tests, and derive reasonable inferences from those tests, many of the challenges of practicing
reproducible science could be resolved with the advent of better tools, meaning resources for
performing analysis, visualizing and capturing workflows, and assessing the reproducibility of a
result.
<!--l. 2490--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">C.1.1   </span> <a 
 id="x42-129000C.1.1"></a>The components of a reproducible analysis</h4>
<!--l. 2492--><p class="noindent" >A reproducible analysis, in its truest definition, must be possible to do again. This means several key
components for the creation and life cycle of the data and methods:
<!--l. 2497--><p class="indent" >
     <dl class="enumerate"><dt class="enumerate">
  1. </dt><dd 
class="enumerate">complete documentation of data derivation, analysis, and structure
     </dd><dt class="enumerate">
  2. </dt><dd 
class="enumerate">machine accessible methods and data resources
     </dd><dt class="enumerate">
  3. </dt><dd 
class="enumerate">automatic integration of data, methods, and standards</dd></dl>
<!--l. 2505--><p class="indent" >   A basic first question is whether it is reasonable to asking for these components from a neuroimaging
research. The last few decades of neuroimaging research have been primarily dominated by collection of
small datasets, and derivation of custom pipelines or manual processing in graphical interfaces
using a standard suite of software [<span 
class="ec-lmbx-10">? ? </span>]. Under these circumstances, it has been shown that
                                                                                   

                                                                                   
results can vary based on operating system [<span 
class="ec-lmbx-10">? </span>], choice of analysis parameters and processing
strategy, and the data itself. The Stanford Center for Reproducible Neuroscience at Stanford
University [<span 
class="ec-lmbx-10">? </span>] founded by Russ Poldrack in 2015 was a first step in addressing the disorganized
nature of neuroimaging analysis working on methods&#x00A0;and tools to instill standards and best
practices [<span 
class="ec-lmbx-10">? </span>] into the production of new results. The center aims to capture the difficulties
associated with the proper analysis of datasets by offering reproducibility as a service. A truly
reproducible analysis requires the collection, processing, documentation, standardization, and
sound evaluation of a well-scoped hypothesis using large data and openly available methods.
From an infrastructural standpoint this extends far beyond requiring expertise in a domain
science and writing skills, calling for prowess in high performance computing, programming,
database and data structure generation and management, and web development. &#x00A0;It may not be
feasible to expect an already heavily burdened academic researcher to provide consistently
standardized, well-documented, and machine accessible products with varying and potentially limited
resources of a university. Further, the limited time of four to five years of graduate training is not
substantial to train both biological domain and informatics experts. Thus, the center offers a
novel framework to take on this burden and offer reproducible analyses as a service. Such a
service would allows for the scientist to focus on the sound acquisition of data, and on his or her
expertise in asking the original biological question. Under such a framework, the list of best
practices for reproducibility can not only be a suggestion, but an implemented, standardized
reality.
<!--l. 2540--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">C.1.2   </span> <a 
 id="x42-130000C.1.2"></a>Documentation</h4>
<!--l. 2542--><p class="noindent" >While an infrastructure that manages data organization and analysis will immediately provide
documentation for workflow, this same standard must trickle into the routine of the average
neuroimaging scientist before and during the collection of the input data. The research process
is not an algorithm, but rather a set of cultural and personal customs that starts from the
generation of new ideas, and encompasses preferences and style in reading papers and taking
notes, and even personal reflection. &#x00A0;Young scientists learn through personal experience and
immersion in highly productive labs with more experienced scientists to advise their learning. A lab
at a prestigious University is like a business that exists only by way of having some success
with producing research products, and so the underlying assumption is that the scientists in
training should follow suit. The unfortunate reality is that the highly competitive nature of
obtaining positions in research means that the composition of a lab tends to weigh heavily in
individuals early in their research careers, with a prime focus on procuring funding for grants
to publish significant results. Thus, it tends to be the case that young scientists know that
                                                                                   

                                                                                   
it&#8217;s important to read papers, take notes, and immerse themselves in their passion, but their
method of doing this is unguided. In this scenario, a reasonable solution is not to place higher
expectation on the already encumbered scientists, but to provide them with tools for learning and
documentation.
<!--l. 2565--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x42-131000C.1.2"></a>Documentation of Code</h5>
<!--l. 2567--><p class="noindent" >In computational fields, it is typically the case that the most direct link to reproducing an analysis is not
perusing through research prose, but by way of obtaining the code. Writing is just idea and hope until
someone has programmed something. Thus, a researcher in a computational field will find it very hard to
be successful if he or she is not comfortable with version control [<span 
class="ec-lmbx-10">? </span>]. Version control keeps a record of all
changes through the life cycle of a project. It allows for the tagging of points in time to different versions of
a piece of software, and going back in time. These elements are essential for reproducible science practices
that are based on sharing of methods and robust documentation of a research process. It takes very
little effort for a researcher to create an account with a version control service (for example,
<a 
href="http://www.github.com" >http://www.github.com</a>), and typically the biggest barriers to this practice are cultural. A researcher
striving to publish novel ideas and methods is naturally going to be concerned over sharing
ideas and methods until they have been given credit for them. This calls for a change not only
in infrastructure, but research culture, and there is likely no way to do that other than by
slow change of incentives with example over time. It should be natural for a researcher, when
starting a new project, to immediately create a repository to organize its life-cycle.&#x00A0;While
we cannot be certain that services like Github, Bitbucket, and Sourceforge are completely
reliable and will exist into infinitum, this basic step can minimally ensure that work is not
lost to a suddenly dead hard-drive, and methods reported in the text of a manuscript can be
immediately found in the language that produced the result. Researchers have much to gain in
using these services. If a computational graduate student is not using and established in using
Github by the end of his or her career, this is a failure in his or her training as a reproducible
scientist.
<!--l. 2599--><p class="indent" >   On the level of documentation in the code itself, this is often a personal, stylistic process that varies by
field. An individual in the field of computer science is more likely to have training in algorithms and proper
use of data structures and advanced programming ideas, and is more likely to produce computationally
efficient applications based on bringing together a cohesive set of functions and objects. We
might say this kind of research scientist, by way of choosing to study computer science to
begin with, might be more driven to develop tools and applications, and unfortunately for
academia will ultimately be most rewarded for pursuing a job in industry. This lack of &#8220;academic
software developers&#8221; is another problem that I address later (Section 5.2), as it is arguably
the prime reason that better, domain-specific, tools do not exist for academic researchers. An
                                                                                   

                                                                                   
epiphany that sometimes takes years to realize is the idea that documentation of applications lives
in the code itself. The design, choice of variable names and data structures, spacing of the
lines and functions, and implementation decisions can render a script easy to understand, or a
mess of characters that can only be understood by walking through each line in an interactive
console. Scientists in training, whether aiming to build elegant tools or simple batch scripts,
should be aware of these subtle choices in the structure of their applications. Cryptic syntax and
non-intuitive processes can be made up for with a (sometimes seemingly) excessive amount of
commenting. Donald Knuth introduced the idea of &#8220;literate programming,&#8221; [<span 
class="ec-lmbx-10">? </span>] a methodology that
includes both code and documentation in a single document. Prime examples include iPython
notebook, R-markdown, and syntax highlighting in online material to combine code and text.
&#x00A0;Whether the researcher adds substantial comments, &#x00A0;uses a literate programming methodology,
or communicates through variable naming and flow of code, the goal is equivalent: to ensure
that a researcher&#8217;s flow of thinking and process is sufficiently represented in his programming
outputs.
<!--l. 2632--><p class="noindent" >
   <h5 class="subsubsectionHead"><a 
 id="x42-132000C.1.2"></a>Visual Documentation</h5>
<!--l. 2634--><p class="noindent" >Documentation can be enhanced with better tools. For example, automated documentation tools (e.g.,
Sphinx for python) [<span 
class="ec-lmbx-10">? </span>] can immediately transform comments hidden away in a Github repository into a
clean, user friendly website for reading about the functions. &#x00A0;Scientists should be provided with these
modern services because arguably, dissemination of a result is just as important (if not more important)
than generation of the result itself. An overlooked component toward understanding of a result is
providing the learner with more than a statistical metric reported in a manuscript, but a cohesive
story to put the result into terms that he or she can relate to. Visualization accompanied with
a story to connect the result to ideas that are easy to synthesize can better distribute the
result to both other researchers and the larger community. For this reason, results cannot only
live in publications, but must be extended to visual documentation such as websites, blogs,
and media sources. What this means is that it might be common practice to, along with a
publication, write a blog post and link to it. This process should be easier than it currently
is, as not all scientists know how to maintain a web presence. For example, a typical poster
presented at a conference might be easily transformed into an&#x00A0;interactive, online poster. &#x00A0;Static
documents such as theses and papers might immediately be parsed for sophisticated natural
language processing applications [<span 
class="ec-lmbx-10">? </span>] to be readily find-able by search engines, and more readily
available for social media discussion. A scientist should be immediately empowered to publish a
domain-specific web report that includes meaningful visualization and prose for an analysis.
Importantly, it must integrate seamlessly into the methodology that it aims to explain, and associated
resources that were used to derive it. &#x00A0;This desired functionality may be beyond the scope of the
                                                                                   

                                                                                   
academic researcher, and development of such resources more suited for a new level of emerging
scientist.
                                                                                   

                                                                                   
   <!--l. 2665--><div class="crosslinks"><p class="noindent">[<a 
href="mainse27.html" >next</a>] [<a 
href="mainap3.html" >prev</a>] [<a 
href="mainap3.html#tailmainap3.html" >prev-tail</a>] [<a 
href="mainse26.html" >front</a>] [<a 
href="mainap3.html#mainse26.html" >up</a>] </p></div>
<!--l. 2665--><p class="indent" >   <a 
 id="tailmainse26.html"></a>    
</body></html> 
